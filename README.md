<div align="center">

# ğŸš— Vehicle Data MLOps Pipeline
### End-to-End Production ML System with CI/CD on AWS

![Python](https://img.shields.io/badge/Python-3.10-3776AB.svg?style=flat&logo=python&logoColor=white)
![FastAPI](https://img.shields.io/badge/FastAPI-005571?style=flat&logo=fastapi)
![AWS](https://img.shields.io/badge/AWS-%23FF9900.svg?style=flat&logo=amazon-aws&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-2496ED.svg?style=flat&logo=docker&logoColor=white)
![MongoDB](https://img.shields.io/badge/MongoDB-%234ea94b.svg?style=flat&logo=mongodb&logoColor=white)
![GitHub Actions](https://img.shields.io/badge/GitHub_Actions-2088FF.svg?style=flat&logo=github-actions&logoColor=white)

</div>

---

## ğŸ“Œ Project Overview

This project demonstrates a **production-grade Machine Learning pipeline** built using modern MLOps principles. It covers the complete ML lifecycle â€” from data ingestion to automated cloud deployment. The system is designed with scalability, maintainability, and automation in mind.

### âœ… Key Features
- Modular ML pipeline architecture
- MongoDB-based data ingestion (development only)
- Data validation & transformation
- Model training & evaluation
- AWS S3 model registry
- Dockerized deployment
- Automated CI/CD using GitHub Actions
- Deployment on AWS EC2

---

## ğŸ—ï¸ Architecture Overview

**Data Flow Pipeline:**
```text
MongoDB Atlas (Dev) â” Data Ingestion â” Data Validation â” Data Transformation â” Model Trainer â” Model Evaluation â” Model Pusher (AWS S3) â” Prediction Pipeline (FastAPI) â” Docker â” ECR â” EC2 (CI/CD)
```
# âš™ï¸ Tech Stack

ğŸ”¹ Backend & ML
Language: Python 3.10

Framework: FastAPI

Libraries: Scikit-learn, Pandas, NumPy, PyYAML

ğŸ”¹ Database
Database: MongoDB Atlas

ğŸ”¹ Cloud Services
AWS S3: Model Registry

AWS ECR: Docker Image Storage

AWS EC2: Deployment environment

AWS IAM: Access Control

ğŸ”¹ DevOps & Automation
Containerization: Docker

CI/CD: GitHub Actions

Runner: Self-hosted Runner on EC2

# ğŸ“‚ Project Structure

```text
src/
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”œâ”€â”€ data_validation.py
â”‚   â”œâ”€â”€ data_transformation.py
â”‚   â”œâ”€â”€ model_trainer.py
â”‚   â”œâ”€â”€ model_evaluation.py
â”œâ”€â”€ entity/
â”œâ”€â”€ configuration/
â”œâ”€â”€ aws_storage/
â”œâ”€â”€ constants/
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ training_pipeline.py
â”‚   â”œâ”€â”€ prediction_pipeline.py
app.py
Dockerfile
requirements.txt
.github/workflows/aws.yaml
```

# ğŸ—„ï¸ MongoDB Setup (Development Only)

MongoDB is used only for local training and experimentation.

Steps:
Create a MongoDB Atlas project

Create an M0 cluster

Add a database user

Allow IP access: 0.0.0.0/0

Copy the connection string

Set Environment Variable:
Bash:

```Bash
export MONGODB_URL="your_connection_string"
PowerShell:

PowerShell
$env:MONGODB_URL="your_connection_string"
```
âš ï¸ Note: MongoDB is NOT required for deployment. Deployment uses only the prediction pipeline and pre-trained models.

# â˜ï¸ AWS Setup

1ï¸âƒ£ IAM User
Programmatic access enabled

AdministratorAccess policy attached

2ï¸âƒ£ S3 Bucket
Bucket Name: your_bucket_name

Region: us-east-1

3ï¸âƒ£ Environment Variables
```Bash
export AWS_ACCESS_KEY_ID="your_key"
export AWS_SECRET_ACCESS_KEY="your_secret"
export AWS_DEFAULT_REGION="us-east-1"
```
# ğŸ³ Docker Setup

Build Image:

```Bash
docker build -t vehicleproj .
```
Run Locally:

```Bash
docker run -p 5000:5000 vehicleproj
```
Application runs at: http://localhost:5000

# ğŸš€ CI/CD Pipeline

Triggered on every push to the main branch. Ensures fully automated, zero-downtime deployments.

Continuous Integration:

Build Docker image

Push image to AWS ECR

Continuous Deployment:

Pull latest image on EC2

Stop previous container

Deploy updated container

# ğŸŒ Production Deployment

Docker image stored in AWS ECR

EC2 Ubuntu server hosts the application

Port 5000 exposed via Security Group

Access the application: http://<EC2_PUBLIC_IP>:5000

# ğŸ” Security Best Practices

âœ”ï¸ AWS credentials stored securely as GitHub Secrets

âœ”ï¸ Environment variables for sensitive configs

âœ”ï¸ No secrets hardcoded in the codebase

âœ”ï¸ .env, artifact/, and local files ignored via .gitignore

# ğŸ§  MLOps Concepts Demonstrated

Modular ML pipeline architecture

Configuration-driven design

Artifact management

Model registry using S3

Docker containerization

CI/CD automation

Cloud-native deployment

# â–¶ï¸ Running Locally

To set up the project on your local machine:

``` Bash
conda create -n vehicle python=3.10 -y
conda activate vehicle
pip install -r requirements.txt
python app.py
```

# ğŸ“ˆ Future Improvements

[ ] Implement real-time Model monitoring

[ ] Add AWS CloudWatch logging

[ ] Create multi-environment deployment (Staging/Production)

[ ] Adopt Infrastructure as Code (Terraform)

[ ] Integrate a model versioning strategy (e.g., MLflow)

# ğŸ‘¨â€ğŸ’» Author
**Ganesh Deshpande** 

Machine Learning Engineer | MLOps Enthusiast Skills: AWS | Docker | FastAPI | CI/CD
